# -*- coding: utf-8 -*-
"""sayli_attempt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sW92YOp-EXrDTVHS5ioMPYLEGZjiWJr8

# Chatbot w/ ChatGPT API
"""

!pip install openai==0.28 PyPDF2

import openai
import PyPDF2
import os
import pandas as pd
import time

!pip install gradio
import gradio as gr

from google.colab import userdata
OPEN_API_KEY = "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"
GEMINI_API_KEY = "AIzaSyCQeU7Nn1LJawGpnaFmkjtuTa0-84H4zhA"
import google.generativeai as genai

filepath= "/content/notes1_personalisedLearninng.pdf"
openai.api_key  = OPEN_API_KEY
genai.configure(api_key=GEMINI_API_KEY)
genai_model = genai.GenerativeModel('gemini-pro')

def get_completion(prompt, model="gpt-3.5-turbo"):
  messages = [{"role": "user", "content": prompt}]
  response = openai.ChatCompletion.create(
     model=model,
     messages=messages,
     temperature=0, # this is the degree of randomness of the model's output
  )
  return response.choices[0].message["content"]

# creating a pdf file object
pdfFileObject = open(filepath, 'rb')
pdfReader = PyPDF2.PdfReader(pdfFileObject)
text=[]
summary=' '
for i in range(0,len(pdfReader.pages)):
  pageObj = pdfReader.pages[i].extract_text()
  pageObj= pageObj.replace('\t\r','')
  pageObj= pageObj.replace('\xa0','')

  text.append(pageObj)

!pip install fuzzywuzzy

import PyPDF2
import spacy
import openai
import sys
from fuzzywuzzy import fuzz  # You can keep this or use an alternative

# Load English tokenizer
nlp = spacy.load("en_core_web_sm")

def get_user_answer():
    answer = input("Please provide your answer: ").strip().lower()
    return answer
    if answer == "exit":
        print("Exiting the program.")
        sys.exit()
       # Terminate the code if user inputs "exit"


# Open the PDF file
pdf_file_path = "/content/notes1_personalisedLearninng.pdf"
with open(pdf_file_path, "rb") as file:
    # Create a PDF reader object
    pdf_reader = PyPDF2.PdfReader(file)

    # Initialize an empty string to store the extracted text
    extracted_text = ""

    # Extract text from each page of the PDF
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        extracted_text += page.extract_text()

# Split the text into chunks
chunk_size = 4000  # Adjust as needed based on model context length
chunks = [extracted_text[i:i+chunk_size] for i in range(0, len(extracted_text), chunk_size)]

# Generate questions and answers for each chunk
all_questions = []
all_answers = []
for chunk in chunks:
    prompt = f"This is a portion of the document content: {chunk}. Generate a relevant question and a concise answer."
    try:
        response = openai.Completion.create(
            engine="gpt-3.5-turbo-instruct",  # Updated model
            prompt=prompt,
            max_tokens=150,  # Adjust as needed
            n=1,
            stop=None,
            temperature=0.7,  # Adjust for creativity vs factuality
        )
    except openai.error.OpenAIError as e:
        print(f"OpenAI Error for chunk: {e}")
        continue  # Skip to the next chunk on error

    # Handle potential unexpected responses
    if response.choices is None or len(response.choices) == 0:
        print("OpenAI failed to generate a response for this chunk. Skipping.")
        continue

    question = response.choices[0].text.strip().split("\n")[0]
    # Ensure answer is retrieved even if not on a separate line
    answer = " ".join(response.choices[0].text.strip().split("\n")[1:])

    if not question or not answer:  # Check for empty question or answer
        print("Skipping chunk due to missing question or answer.")
        continue


    all_questions.append(question)
    all_answers.append(answer)

# Process and evaluate answers
with open("incorrect_answers.txt", "a") as incorrect_answers_file:
    for i, (question, answer) in enumerate(zip(all_questions, all_answers)):
        print(f"Question {i+1}: {question}")
        student_answer = get_user_answer()
        similarity = fuzz.ratio(student_answer, answer)  # Use fuzzywuzzy or alternative
        threshold = 50  # Adjust threshold for acceptable similarity

        if similarity >= threshold:
            print("Your answer is correct!")
        else:
            print("Your answer is incorrect.")
            incorrect_answers_file.write(f"Question {i+1}:\n")
            incorrect_answers_file.write(f"Your answer was incorrect: {student_answer}\n")
            incorrect_answers_file.write(f"Correct answer: {answer}\n\n")
            print(f"Correct answer: {answer}\n\n")

import gradio as gr
import openai
from fuzzywuzzy import fuzz

def generate_question_and_evaluate(answer):
    # Generate a question and answer for the provided text
    prompt = f"This is a portion of the document content: {answer}. Generate a relevant question and a concise answer."

    try:
        response = openai.Completion.create(
            engine="gpt-3.5-turbo-instruct",  # Updated model
            prompt=prompt,
            max_tokens=150,  # Adjust as needed
            n=1,
            stop=None,
            temperature=0.7,  # Adjust for creativity vs factuality
        )
    except openai.error.OpenAIError as e:
        return "OpenAI Error. Please try again."

    if response.choices is None or len(response.choices) == 0:
        return "OpenAI failed to generate a response. Please try again."

    generated_question = response.choices[0].text.strip().split("\n")[0]
    generated_answer = " ".join(response.choices[0].text.strip().split("\n")[1:])

    # Evaluate the user's answer
    similarity = fuzz.ratio(answer, generated_answer)
    threshold = 80  # Adjust threshold for acceptable similarity

    result = f"Generated Question: {generated_question}\n\nPlease answer the following question:\n\nGenerated Question: {generated_question}\n\nYour Answer: {answer}\n"

    if similarity >= threshold:
        result += "Your answer is correct!"
    else:
        result += f"Your answer is incorrect.\nCorrect answer: {generated_answer}"

    return result

iface = gr.Interface(fn=generate_question_and_evaluate, inputs="text", outputs="text", live=True)
iface.launch()

import gradio as gr

def generate_question_and_evaluate(answer):
    # Generate a question and answer for the provided text
    prompt = f"This is a portion of the document content: {answer}. Generate a relevant question and a concise answer."

    try:
        response = openai.Completion.create(
            engine="gpt-3.5-turbo-instruct",  # Updated model
            prompt=prompt,
            max_tokens=150,  # Adjust as needed
            n=1,
            stop=None,
            temperature=0.7,  # Adjust for creativity vs factuality
        )
    except openai.error.OpenAIError as e:
        return "OpenAI Error. Please try again."

    if response.choices is None or len(response.choices) == 0:
        return "OpenAI failed to generate a response. Please try again."

    generated_question = response.choices[0].text.strip().split("\n")[0]
    generated_answer = " ".join(response.choices[0].text.strip().split("\n")[1:])

    # Evaluate the user's answer
    similarity = fuzz.ratio(answer, generated_answer)
    threshold = 80  # Adjust threshold for acceptable similarity

    result = f"Generated Question: {generated_question}\nYour Answer: {answer}\n"

    if similarity >= threshold:
        result += "Your answer is correct!"
    else:
        result += f"Your answer is incorrect.\nCorrect answer: {generated_answer}"

    return result

iface = gr.Interface(fn=generate_question_and_evaluate, inputs="text", outputs="text")
iface.launch()